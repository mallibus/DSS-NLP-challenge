{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge categories\n",
    "\n",
    "1. Economy\n",
    "2. Regulation\n",
    "3. Environment\n",
    "4. Health related issue\n",
    "5. Industry\n",
    "6. Cultures and customs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual research for Wikipedia categories\n",
    "p_wiki = wiki_wiki.page('travels')\n",
    "p_wiki.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Travel is the movement of people between distant geographical locations. Travel can be done by foot, bicycle, automobile, train, boat, bus, airplane, ship or other means, with or without luggage, and can be one way or round trip. Travel can also include relatively short stays between successive movements.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Category:Articles with Curlie links': Category:Articles with Curlie links (id: ??, ns: 14),\n",
       " 'Category:Tourism': Category:Tourism (id: ??, ns: 14),\n",
       " 'Category:Tourist activities': Category:Tourist activities (id: ??, ns: 14),\n",
       " 'Category:Transport culture': Category:Transport culture (id: ??, ns: 14),\n",
       " 'Category:Travel': Category:Travel (id: ??, ns: 14),\n",
       " 'Category:Webarchive template wayback links': Category:Webarchive template wayback links (id: ??, ns: 14),\n",
       " 'Category:Wikipedia articles with GND identifiers': Category:Wikipedia articles with GND identifiers (id: ??, ns: 14),\n",
       " 'Category:Wikipedia articles with HDS identifiers': Category:Wikipedia articles with HDS identifiers (id: ??, ns: 14),\n",
       " 'Category:Wikipedia articles with LCCN identifiers': Category:Wikipedia articles with LCCN identifiers (id: ??, ns: 14),\n",
       " 'Category:Wikipedia articles with NARA identifiers': Category:Wikipedia articles with NARA identifiers (id: ??, ns: 14),\n",
       " 'Category:Wikipedia articles with NDL identifiers': Category:Wikipedia articles with NDL identifiers (id: ??, ns: 14),\n",
       " 'Category:Wikipedia indefinitely semi-protected pages': Category:Wikipedia indefinitely semi-protected pages (id: ??, ns: 14)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(p_wiki.summary)\n",
    "p_wiki.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Economy','Regulation','Environment','Health','Industry','Cultures']\n",
    "wiki_scan = [[\"Category:Economy\",'Category:Stock market','Category:Trade','Category:Monetary economics'],\n",
    "             ['Category:Regulation','Category:Public policy','Category:Economics of regulation'],\n",
    "             ['Category:Environmental science','Category:Ecology','Category:Environmentalism'],\n",
    "             ['Category:Health','Category:Physical fitness','Category:Medicine'],\n",
    "             ['Category:Industry','Category:Manufacturing','Category:Energy','Category:Technology'],\n",
    "             ['Category:Culture','Category:Social concepts','Category:Tourism','Category:Tourist activities']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_categorymembers(categorymembers, level=0, max_level=1,members=[]):\n",
    "    for c in categorymembers.values():\n",
    "        #print(\"%s: %s (ns: %d)\" % (\"*\" * (level + 1), c.title, c.ns))\n",
    "        if c.ns == wikipediaapi.Namespace.CATEGORY and level < max_level:\n",
    "            #print(\"down cat - %s: %s (ns: %d)\" % (\"*\" * (level + 1), c.title, c.ns))\n",
    "            members = add_categorymembers(c.categorymembers, level=level + 1, max_level=max_level,members=members)\n",
    "            #print(\"Down:\",len(members))\n",
    "        else:\n",
    "            #print(\"append - %s: %s (ns: %d)\" % (\"*\" * (level + 1), c.title, c.ns))\n",
    "            #if len(c.text)>0:\n",
    "            members.append(c.title)\n",
    "            #else:\n",
    "            #    print(\"ZERO - %s: %s (ns: %d)\" % (\"*\" * (level + 1), c.title, c.ns))\n",
    "            \n",
    "            #print(\"A:\",len(members))\n",
    "    return members\n",
    "\n",
    "def list_categorymembers(categories, level=0, max_level=1):\n",
    "    l = []\n",
    "    for cat_name in categories:\n",
    "        cat = wiki_wiki.page(cat_name)\n",
    "        l_cat = (add_categorymembers(cat.categorymembers, level=0, max_level=1,members=[]))\n",
    "        print(f'{cat_name} : {len(l_cat)} elements')\n",
    "        l.extend(l_cat)\n",
    "    return l\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Economy ['Category:Economy', 'Category:Stock market', 'Category:Trade', 'Category:Monetary economics']\n",
      "Category:Economy : 703 elements\n",
      "Category:Stock market : 884 elements\n",
      "Category:Trade : 804 elements\n",
      "Category:Monetary economics : 575 elements\n",
      "Pages to scan : 2966\n",
      "Regulation ['Category:Regulation', 'Category:Public policy', 'Category:Economics of regulation']\n",
      "Category:Regulation : 456 elements\n",
      "Category:Public policy : 1076 elements\n",
      "Category:Economics of regulation : 284 elements\n",
      "Pages to scan : 1816\n",
      "Environment ['Category:Environmental science', 'Category:Ecology', 'Category:Environmentalism']\n",
      "Category:Environmental science : 1377 elements\n",
      "Category:Ecology : 1822 elements\n",
      "Category:Environmentalism : 603 elements\n",
      "Pages to scan : 3802\n",
      "Health ['Category:Health', 'Category:Physical fitness', 'Category:Medicine']\n",
      "Category:Health : 2870 elements\n",
      "Category:Physical fitness : 9 elements\n",
      "Category:Medicine : 1492 elements\n",
      "Pages to scan : 4371\n",
      "Industry ['Category:Industry', 'Category:Manufacturing', 'Category:Energy', 'Category:Technology']\n",
      "Category:Industry : 1378 elements\n",
      "Category:Manufacturing : 868 elements\n",
      "Category:Energy : 1779 elements\n",
      "Category:Technology : 2578 elements\n",
      "Pages to scan : 6603\n",
      "Cultures ['Category:Culture', 'Category:Social concepts', 'Category:Tourism', 'Category:Tourist activities']\n",
      "Category:Culture : 2839 elements\n",
      "Category:Social concepts : 1779 elements\n",
      "Category:Tourism : 1295 elements\n",
      "Category:Tourist activities : 130 elements\n",
      "Pages to scan : 6043\n"
     ]
    }
   ],
   "source": [
    "wiki_pages = dict()\n",
    "for c,l in zip(categories,wiki_scan):\n",
    "    print(c,l)\n",
    "    wiki_pages[c] = list_categorymembers(l)\n",
    "    print(f'Pages to scan : {len(wiki_pages[c])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25601"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(wiki_pages[c]) for c in wiki_pages])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from progress import progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words       = 20000     # max number of words in the vocab\n",
    "min_text_words  = 20        # dont'store intpo corpus pages with less than that number of workds\n",
    "max_i           = 10        # number of files scanned per category\n",
    "max_i           = 5         # number of files scanned per category\n",
    "max_i           = 200      # number of files scanned per category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning 201 articles out of 2966 on Economy\n",
      "[============================================================] 100.0% ...200\n",
      "Adding 116 texts into corpus\n",
      "\n",
      "Scanning 201 articles out of 1816 on Regulation\n",
      "[============================================================] 100.0% ...200\n",
      "Adding 184 texts into corpus\n",
      "\n",
      "Scanning 201 articles out of 3802 on Environment\n",
      "[============================================================] 100.0% ...200\n",
      "Adding 201 texts into corpus\n",
      "\n",
      "Scanning 201 articles out of 4371 on Health\n",
      "[============================================================] 100.0% ...200\n",
      "Adding 64 texts into corpus\n",
      "\n",
      "Scanning 201 articles out of 6603 on Industry\n",
      "[============================================================] 100.0% ...200\n",
      "Adding 76 texts into corpus\n",
      "\n",
      "Scanning 201 articles out of 6043 on Cultures\n",
      "[============================================================] 100.0% ...200\n",
      "Adding 80 texts into corpus\n"
     ]
    }
   ],
   "source": [
    "corpus_texts  =  []\n",
    "corpus_cats   =  []\n",
    "for c in wiki_pages.keys():\n",
    "    texts = []\n",
    "    print(f'\\nScanning {max_i+1} articles out of {len(wiki_pages[c])} on {c}')\n",
    "    for i,p in enumerate(wiki_pages[c]):\n",
    "        progress(i, min(max_i,len(wiki_pages[c])), status=f'{i}')\n",
    "        p_wiki = wiki_wiki.page(p)\n",
    "        text = p_wiki.text\n",
    "        if len(text.split())>min_text_words:\n",
    "            texts.append(p_wiki.text)\n",
    "        if i>=max_i:\n",
    "            break\n",
    "    print(f'\\nAdding {len(texts)} texts into corpus')\n",
    "    corpus_texts.extend(texts)\n",
    "    corpus_cats.extend([categories.index(c)]*len(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "t  = Tokenizer(num_words=max_words,oov_token='xxxunk')\n",
    "t.fit_on_texts(corpus_texts)\n",
    "X_corpus = t.texts_to_sequences(corpus_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x1856631a240>"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = {     'max_i'     : max_i,\n",
    "            'max_words' : max_words,\n",
    "            'word_index': t.word_index,\n",
    "            'categories': categories,\n",
    "            'X'         : X_corpus,\n",
    "            'y'         : corpus_cats }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path = Path('dataset/wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dss_wiki_00200_20K1.json'"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f'dss_wiki_{max_i:05}_{max_words//1000}K1.json'\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(path/filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dss, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('dataset/wiki/dss_wiki_00200_20K1.json')"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path/filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dss_wiki_00200_20K1_tokenizer.pkl'"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_filename = f'dss_wiki_{max_i:05}_{max_words//1000}K1_tokenizer.pkl'\n",
    "tokenizer_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(path/tokenizer_filename, 'wb') as f:\n",
    "    serial_grades = pickle.dump(t,f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset of 721 texts of 6 classes - vocabulary of 47380 words\n",
      "['Economy', 'Regulation', 'Environment', 'Health', 'Industry', 'Cultures']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset of {len(dss['X'])} texts of {len(set(dss['y']))} classes - vocabulary of {len(dss['word_index'])} words\")\n",
    "print(dss['categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words = len(dss['word_index'])\n",
    "max_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_arr = np.array(dss['X'])\n",
    "y_arr = np.array(dss['y'])\n",
    "id_to_word = {dss['word_index'][key]:key for key in dss['word_index'].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text #374 - Environment\n",
      "in situ chemical reduction iscr is a new type of environmental remediation technique used for soil and or groundwater\n",
      "remediation to reduce the concentrations of targeted environmental contaminants to acceptable levels it is the mirror\n",
      "process of in situ chemical oxidation isco iscr is usually applied in the environment by xxxunk chemically reductive\n",
      "additives in liquid form into the contaminated area or placing a solid medium of chemical reductants in the path of a\n",
      "contaminant plume it can be used to remediate a variety of organic compounds including some that are resistant to\n",
      "natural degradation the in situ in iscr is just latin for in place signifying that iscr is a chemical reduction reaction\n",
      "that occurs at the site of the contamination like isco it is able to xxxunk many compounds and in theory iscr could be\n",
      "more effective in ground water remediation than isco chemical reduction is one half of a redox reaction which results in\n",
      "the gain of electrons one of the xxxunk in the reaction becomes oxidized or loses electrons while the other reactant\n",
      "becomes reduced or gains electrons in iscr reducing compounds compounds that accept electrons given by other compounds\n",
      "in a reaction are used to change the contaminants into harmless compounds history iscr is a relatively new type of\n",
      "ground water remediation technology the most work on this method of remediation has been done in the past 10–15 years so\n",
      "there are still many gaps in the understanding of the chemistry behind this process the development of iscr started out\n",
      "when k h xxxunk conducted research with zero valent copper and iron in the late 1970s he was able to treat a number of\n",
      "different chlorinated substances such as ddt xxxunk xxxunk and xxxunk to name a few his work has been the basis of iscr\n",
      "today in the 1990s xxxunk xxxunk xxxunk xxxunk and xxxunk all made significant contributions in testing different metals\n",
      "and oxides for the use of iscr xxxunk and xxxunk in particular applied the reductive chemistry to groundwater treatment\n",
      "with the xxxunk of zvi barriers although it has been shown that other metals like aluminum and magnesium can produce the\n",
      "same effect in the laboratory ground water treatment most generally focuses on the use of iron other major contributions\n",
      "in this field includes xxxunk who researched nanoscale iron and xxxunk who researched zero valent iron clay zvi clay\n",
      "this past decade more aspects of iscr have been researched and new methods of implementation such as zvi clay and xxxunk\n",
      "zvi ezvi have been created scientists have also found that certain iron minerals like green rust xxxunk and pyrite also\n",
      "have reductive capabilities although they contain ferrous iron rather than zvi reductants zero valent metals zvms zero\n",
      "valent metals are the main reductants used in iscr the most common metal used is iron in the form of zvi zero valent\n",
      "iron and it is also the metal longest in use however some studies show that zero valent zinc xxxunk could actually be up\n",
      "to ten times more effective at eradicating the contaminants than zvi some applications of zvms are to clean up\n",
      "trichloroethylene tce and hexavalent chromium xxxunk vi zvms are usually implemented by a permeable reactive barrier for\n",
      "example iron that has been embedded in a xxxunk organically modified silica creates a permanent soft barrier underground\n",
      "to capture and reduce small organic compounds as groundwater passes through it iron minerals there are also many iron\n",
      "minerals that can actively be used in xxxunk these minerals use fe2 particular minerals that can be used include green\n",
      "rust xxxunk pyrite and xxxunk the most reactive of the iron minerals are the iron sulfides and oxides pyrite an iron\n",
      "sulfide is able to xxxunk carbon xxxunk in suspension these substances are very interesting because they are naturally\n",
      "present and learning about how they produce reductive zones could lead to the development of better reductants for iscr\n",
      "polysulfides polysulfides are compounds that have chains of sulfur atoms this is a relatively new reactant but it has\n",
      "been tested on the field in treating tce and in comparison to ehc the use of polysulfides is a type of abiotic reduction\n",
      "and works best in anaerobic conditions where iron iii is available the benefit of using polysulfides is that they do not\n",
      "produce any biological waste products however the reaction rates are slow and they require more time to create the\n",
      "xxxunk dual valent iron minerals that are needed for the reduction to occur dithionite dithionite xxxunk can also be\n",
      "used as a reductant it is usually used in addition to iron reduce contaminants a number of reactions take place and\n",
      "eventually the contaminant is removed in the process xxxunk is consumed and the final product of all the reactions is 2\n",
      "sulfur dioxide anions the dithionite is not stable for a long period of time bimetallic materials bimetallic materials\n",
      "are materials that are made out of two different metals or alloys that are tightly bonded together a good example of a\n",
      "bimetallic material would be a bimetallic strip which is used in some kinds of xxxunk in iscr bimetallic materials are\n",
      "small pieces of metals that are coated lightly with a catalyst such as xxxunk silver or platinum the catalyst drives a\n",
      "faster reaction and the small size of the particles allows them to effectively move into and remain in the target zone\n",
      "proprietary materials one proprietary material present today for iscr is the ehc technology created by adventus this\n",
      "particular product is actually a mixture of carbon nutrients and zero valent iron the theory behind this product is that\n",
      "the carbon in the mixture will promote bacterial growth in the subsurface the growing bacteria consume oxygen which\n",
      "easily accepts electrons present in the subsurface which increases reducing potential the growing bacteria also xxxunk\n",
      "and produce fatty acids that act as electron donors to other bacteria and substances adventus uses this combination of\n",
      "biotic and abiotic processes to implement iscr ehc is injected as a xxxunk a mixture that is 15 to 40 solids and weight\n",
      "with the rest being liquid into the substratum another material worth mentioning is ezvi xxxunk zvi which is a nasa\n",
      "technology ezvi is used mainly to treat halogenated hydrocarbons and xxxunk ezvi is nanoscale iron that is placed into a\n",
      "biodegradable oil xxxunk the xxxunk is then injected into the substratum reactions in iscr reductive processes in iscr\n",
      "there are many reductive processes that can take place there are xxxunk β elimination xxxunk α elimination and electron\n",
      "transfer the specific combination of reductive processes that actually take place in the subsurface depends on the\n",
      "species of contaminant that is present and also the type of reduction being used the natural and biological processes\n",
      "that take place in the substratum also affect the kinds of reductive processes that are found surface xxxunk reactions\n",
      "the reactions that occur with permeable reactive barriers and ferrous iron are surface based the surface reactions take\n",
      "three different forms direct reduction electron xxxunk through ferrous iron and reduction by production and reaction of\n",
      "hydrogen pathway a represents direct electron transfer et for fe0 to the adsorbed xxxunk rx at the metal water point of\n",
      "contact resulting in xxxunk and production of fe2 pathway b shows that fe2 resulting from corrosion of fe0 may also\n",
      "xxxunk rx producing xxxunk pathway c shows that h2 from the anaerobic corrosion of fe2 might react with rx if a catalyst\n",
      "is present enhancement of reductive pathways the reductive processes discussed above can be enhanced in two ways one is\n",
      "by increasing the amount of usable iron in the subsurface to increase the rate of the reduction by chemical or\n",
      "biological means the second method is to enhance the reducing ability of the iron by coupling it with other chemical\n",
      "reductants or using biological reduction with it using this processes scientists combined sodium dithionite with iron to\n",
      "treat xxxunk vi and tce effectively combining bacterial action and biological processes with iron is also known to be\n",
      "effective the most evident uses of biological processes are with the ezvi technology created by nasa and with the ehc\n",
      "product created by adventus both of these materials have iron within some biological matrix iron is suspended in\n",
      "vegetable oil in ezvi and in organic carbon in ehc and use microbial organisms to enhance the reduction zone and to\n",
      "create a more anaerobic environment for the reactions to take place in implementation the most common type of\n",
      "implementation of iscr is the installation of permeable reactive barriers prbs but there are instances when the\n",
      "reductant can be directly injected into the subsurface to treat source areas semi permeable reactive barrier these\n",
      "barriers are usually made out of zero valent iron zvi but can also be made with any other zero valent metal the most\n",
      "common way they are made is by filling a xxxunk with zvi nanoscale iron or xxxunk nanoscale iron particles can also be\n",
      "injected directly into the subsurface to treat xxxunk and they have large surface areas and therefore high xxxunk and\n",
      "can be distributed more evenly in the contamination site xxxunk reaction rates are rapid the main advantages of prbs are\n",
      "that it can reduce many a variety of contaminants and it has no above ground structure problems with prbs include that\n",
      "even with well constructed barriers there might be the problem of hydraulic short xxxunk direct injection of reductants\n",
      "nanoscale iron can be directly into the subsurface because they are small enough to be distributed thoroughly because\n",
      "the particles are so small they have a comparatively large reactive surface providing a more effective reaction as of\n",
      "now nanoscale iron is the only material that has been used with this injection strategy and it is probably the only\n",
      "material that is effective in injection future of iscr iscr is a relatively new technology so there much scope for\n",
      "research and improvement right now although the reactions that make up iscr have been studied extensively there is not\n",
      "much background on what factors most contribute to the effectiveness of iscr one thing that needs to be done is find out\n",
      "exactly what reactions are taking place in the subsurface iscr is fairly more complex than isco because there are\n",
      "substances in the subsurface that will naturally reduce contaminants the pathways that a contaminant can go through are\n",
      "consequently more diverse also questions that need to be kept in mind are which reducing agent will work best with a\n",
      "particular xxxunk how can we improve the iscr technology that is already commercially available references external\n",
      "links additional information on this topic may be found at the following sites adventus group\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=np.random.randint(len(X_arr))\n",
    "\n",
    "print(f'Text #{i} - {dss[\"categories\"][y_arr[i]]}')\n",
    "s = ' '.join(id_to_word[id] for id in X_arr[i])\n",
    "print('\\n'.join(textwrap.wrap(s, width=120, replace_whitespace=False)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_wiki = 'https://drive.google.com/uc?export=download&id=1iteaiSPd1OLJdKZitcv76LzrD_mAC8S8'# 5\n",
    "url_wiki = 'https://drive.google.com/uc?export=download&id=15hhpN2EszdRx7-PN43yGFsoFPuZzwFO1'   # 200\n",
    "url_wiki = 'https://drive.google.com/file/d/1HZxMVthzG-tboMXO0qsPkeH2Cuq1YCbi/view?usp=sharing' # 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-484-9ff0aad810f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_wiki\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Dataset of {len(dss['X'])} texts of {len(set(dss['y']))} classes - vocabulary of {len(dss['word_index'])} words\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'categories'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf20\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf20\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf20\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \"\"\"\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf20\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "f = urllib.request.urlopen(url_wiki)\n",
    "dss = json.load(f)\n",
    "print(f\"Dataset of {len(dss['X'])} texts of {len(set(dss['y']))} classes - vocabulary of {len(dss['word_index'])} words\")\n",
    "print(dss['categories'])\n",
    "      \n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
