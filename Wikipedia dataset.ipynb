{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge categories\n",
    "\n",
    "1. Economy\n",
    "2. Regulation\n",
    "3. Environment\n",
    "4. Health related issue\n",
    "5. Industry\n",
    "6. Cultures and customs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual research for Wikipedia categories\n",
    "p_wiki = wiki_wiki.page('travels')\n",
    "p_wiki.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Travel is the movement of people between distant geographical locations. Travel can be done by foot, bicycle, automobile, train, boat, bus, airplane, ship or other means, with or without luggage, and can be one way or round trip. Travel can also include relatively short stays between successive movements.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Category:Articles with Curlie links': Category:Articles with Curlie links (id: ??, ns: 14),\n",
       " 'Category:Tourism': Category:Tourism (id: ??, ns: 14),\n",
       " 'Category:Tourist activities': Category:Tourist activities (id: ??, ns: 14),\n",
       " 'Category:Transport culture': Category:Transport culture (id: ??, ns: 14),\n",
       " 'Category:Travel': Category:Travel (id: ??, ns: 14),\n",
       " 'Category:Webarchive template wayback links': Category:Webarchive template wayback links (id: ??, ns: 14),\n",
       " 'Category:Wikipedia articles with GND identifiers': Category:Wikipedia articles with GND identifiers (id: ??, ns: 14),\n",
       " 'Category:Wikipedia articles with HDS identifiers': Category:Wikipedia articles with HDS identifiers (id: ??, ns: 14),\n",
       " 'Category:Wikipedia articles with LCCN identifiers': Category:Wikipedia articles with LCCN identifiers (id: ??, ns: 14),\n",
       " 'Category:Wikipedia articles with NARA identifiers': Category:Wikipedia articles with NARA identifiers (id: ??, ns: 14),\n",
       " 'Category:Wikipedia articles with NDL identifiers': Category:Wikipedia articles with NDL identifiers (id: ??, ns: 14),\n",
       " 'Category:Wikipedia indefinitely semi-protected pages': Category:Wikipedia indefinitely semi-protected pages (id: ??, ns: 14)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(p_wiki.summary)\n",
    "p_wiki.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Economy','Regulation','Environment','Health','Industry','Cultures']\n",
    "wiki_scan = [[\"Category:Economy\",'Category:Stock market','Category:Trade','Category:Monetary economics'],\n",
    "             ['Category:Regulation','Category:Public policy','Category:Economics of regulation'],\n",
    "             ['Category:Environmental science','Category:Ecology','Category:Environmentalism'],\n",
    "             ['Category:Health','Category:Physical fitness','Category:Medicine'],\n",
    "             ['Category:Industry','Category:Manufacturing','Category:Energy','Category:Technology'],\n",
    "             ['Category:Culture','Category:Social concepts','Category:Tourism','Category:Tourist activities']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_categorymembers(categorymembers, level=0, max_level=1,members=[]):\n",
    "    for c in categorymembers.values():\n",
    "        #print(\"%s: %s (ns: %d)\" % (\"*\" * (level + 1), c.title, c.ns))\n",
    "        if c.ns == wikipediaapi.Namespace.CATEGORY and level < max_level:\n",
    "            #print(\"down cat - %s: %s (ns: %d)\" % (\"*\" * (level + 1), c.title, c.ns))\n",
    "            members = add_categorymembers(c.categorymembers, level=level + 1, max_level=max_level,members=members)\n",
    "            #print(\"Down:\",len(members))\n",
    "        else:\n",
    "            #print(\"append - %s: %s (ns: %d)\" % (\"*\" * (level + 1), c.title, c.ns))\n",
    "            #if len(c.text)>0:\n",
    "            members.append(c.title)\n",
    "            #else:\n",
    "            #    print(\"ZERO - %s: %s (ns: %d)\" % (\"*\" * (level + 1), c.title, c.ns))\n",
    "            \n",
    "            #print(\"A:\",len(members))\n",
    "    return members\n",
    "\n",
    "def list_categorymembers(categories, level=0, max_level=1):\n",
    "    l = []\n",
    "    for cat_name in categories:\n",
    "        cat = wiki_wiki.page(cat_name)\n",
    "        l_cat = (add_categorymembers(cat.categorymembers, level=0, max_level=1,members=[]))\n",
    "        print(f'{cat_name} : {len(l_cat)} elements')\n",
    "        l.extend(l_cat)\n",
    "    return l\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Economy ['Category:Economy', 'Category:Stock market', 'Category:Trade', 'Category:Monetary economics']\n",
      "Category:Economy : 703 elements\n",
      "Category:Stock market : 884 elements\n",
      "Category:Trade : 804 elements\n",
      "Category:Monetary economics : 575 elements\n",
      "Pages to scan : 2966\n",
      "Regulation ['Category:Regulation', 'Category:Public policy', 'Category:Economics of regulation']\n",
      "Category:Regulation : 456 elements\n",
      "Category:Public policy : 1076 elements\n",
      "Category:Economics of regulation : 284 elements\n",
      "Pages to scan : 1816\n",
      "Environment ['Category:Environmental science', 'Category:Ecology', 'Category:Environmentalism']\n",
      "Category:Environmental science : 1377 elements\n",
      "Category:Ecology : 1822 elements\n",
      "Category:Environmentalism : 603 elements\n",
      "Pages to scan : 3802\n",
      "Health ['Category:Health', 'Category:Physical fitness', 'Category:Medicine']\n",
      "Category:Health : 2870 elements\n",
      "Category:Physical fitness : 9 elements\n",
      "Category:Medicine : 1492 elements\n",
      "Pages to scan : 4371\n",
      "Industry ['Category:Industry', 'Category:Manufacturing', 'Category:Energy', 'Category:Technology']\n",
      "Category:Industry : 1378 elements\n",
      "Category:Manufacturing : 868 elements\n",
      "Category:Energy : 1779 elements\n",
      "Category:Technology : 2578 elements\n",
      "Pages to scan : 6603\n",
      "Cultures ['Category:Culture', 'Category:Social concepts', 'Category:Tourism', 'Category:Tourist activities']\n",
      "Category:Culture : 2839 elements\n",
      "Category:Social concepts : 1779 elements\n",
      "Category:Tourism : 1295 elements\n",
      "Category:Tourist activities : 130 elements\n",
      "Pages to scan : 6043\n"
     ]
    }
   ],
   "source": [
    "wiki_pages = dict()\n",
    "for c,l in zip(categories,wiki_scan):\n",
    "    print(c,l)\n",
    "    wiki_pages[c] = list_categorymembers(l)\n",
    "    print(f'Pages to scan : {len(wiki_pages[c])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25601"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(wiki_pages[c]) for c in wiki_pages])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from progress import progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words       = 20000     # max number of words in the vocab\n",
    "min_text_words  = 20        # dont'store intpo corpus pages with less than that number of workds\n",
    "max_i           = 10        # number of files scanned per category\n",
    "max_i           = 5         # number of files scanned per category\n",
    "max_i           = 200      # number of files scanned per category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning 201 articles out of 2966 on Economy\n",
      "[============================================================] 100.0% ...200\n",
      "Adding 116 texts into corpus\n",
      "\n",
      "Scanning 201 articles out of 1816 on Regulation\n",
      "[============================================================] 100.0% ...200\n",
      "Adding 184 texts into corpus\n",
      "\n",
      "Scanning 201 articles out of 3802 on Environment\n",
      "[============================================================] 100.0% ...200\n",
      "Adding 201 texts into corpus\n",
      "\n",
      "Scanning 201 articles out of 4371 on Health\n",
      "[============================================================] 100.0% ...200\n",
      "Adding 64 texts into corpus\n",
      "\n",
      "Scanning 201 articles out of 6603 on Industry\n",
      "[============================================================] 100.0% ...200\n",
      "Adding 76 texts into corpus\n",
      "\n",
      "Scanning 201 articles out of 6043 on Cultures\n",
      "[============================================================] 100.0% ...200\n",
      "Adding 80 texts into corpus\n"
     ]
    }
   ],
   "source": [
    "corpus_texts  =  []\n",
    "corpus_cats   =  []\n",
    "for c in wiki_pages.keys():\n",
    "    texts = []\n",
    "    print(f'\\nScanning {max_i+1} articles out of {len(wiki_pages[c])} on {c}')\n",
    "    for i,p in enumerate(wiki_pages[c]):\n",
    "        progress(i, min(max_i,len(wiki_pages[c])), status=f'{i}')\n",
    "        p_wiki = wiki_wiki.page(p)\n",
    "        text = p_wiki.text\n",
    "        if len(text.split())>min_text_words:\n",
    "            texts.append(p_wiki.text)\n",
    "        if i>=max_i:\n",
    "            break\n",
    "    print(f'\\nAdding {len(texts)} texts into corpus')\n",
    "    corpus_texts.extend(texts)\n",
    "    corpus_cats.extend([categories.index(c)]*len(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "t  = Tokenizer(num_words=max_words,oov_token='xxxunk')\n",
    "t.fit_on_texts(corpus_texts)\n",
    "X_corpus = t.texts_to_sequences(corpus_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = {     'max_i'     : max_i,\n",
    "            'max_words' : max_words,\n",
    "            'word_index': t.word_index,\n",
    "            'categories': categories,\n",
    "            'X'         : X_corpus,\n",
    "            'y'         : corpus_cats }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path = Path('dataset/wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dss_wiki_00200_20K1.json'"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f'dss_wiki_{max_i:05}_{max_words//1000}K1.json'\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(path/filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dss, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('dataset/wiki/dss_wiki_00200_20K1.json')"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path/filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset of 721 texts of 6 classes - vocabulary of 47380 words\n",
      "['Economy', 'Regulation', 'Environment', 'Health', 'Industry', 'Cultures']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset of {len(dss['X'])} texts of {len(set(dss['y']))} classes - vocabulary of {len(dss['word_index'])} words\")\n",
    "print(dss['categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words = len(dss['word_index'])\n",
    "max_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_arr = np.array(dss['X'])\n",
    "y_arr = np.array(dss['y'])\n",
    "id_to_word = {dss['word_index'][key]:key for key in dss['word_index'].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text #15 - Economy\n",
      "this is a list of economic crisis and depressions 1st century the financial panic of ad 33 the result of the mass\n",
      "issuance of xxxunk loans by main roman banking houses 3rd century crisis of the third century 14th century 14th century\n",
      "banking crisis the crash of the xxxunk and the bardi family xxxunk dei bardi in xxxunk 17th century xxxunk und xxxunk\n",
      "xxxunk financial crisis at start of thirty years' war tulip mania xxxunk the general crisis xxxunk arguably the largest\n",
      "worldwide crisis in history 18th century great tobacco depression xxxunk united states south sea bubble 1720 uk\n",
      "mississippi company 1720 france crisis of 1763 – started in amsterdam begun by the collapse of xxxunk xxxunk de xxxunk\n",
      "and johann ernst xxxunk spread to germany and scandinavia great east indian bengal bubble crash 1769 india crash started\n",
      "by rapid xxxunk of east india company crisis of 1772 – started in london and amsterdam begun by the collapse of the\n",
      "bankers neal james xxxunk and down war of american independence financing crisis 1776 united states the french\n",
      "revolution was initiated by its 1 4 billion xxxunk investment here spain invested 700 million xxxunk into fighting panic\n",
      "of 1785 – united states panic of 1792 – united states panic of xxxunk – britain and united states 19th century danish\n",
      "state bankruptcy of 1813 post napoleonic depression post 1815 england panic of 1819 a u s recession with bank failures\n",
      "culmination of u s 's first boom to bust economic cycle panic of 1825 a pervasive british recession in which many banks\n",
      "failed nearly including the bank of england panic of 1837 a u s recession with bank failures followed by a 5 year\n",
      "depression panic of 1847 started as a collapse of british financial markets associated with the end of the 1840s railway\n",
      "industry boom panic of 1857 a u s recession with bank failures panic of 1866 was an international financial downturn\n",
      "that accompanied the failure of xxxunk xxxunk and company in london great depression of british agriculture 1873–1896\n",
      "long depression 1873–1896 panic of 1873 a us recession with bank failures followed by a four year depression panic of\n",
      "1884 panic of 1890 panic of 1893 a us recession with bank failures australian banking crisis of 1893 panic of 1896 20th\n",
      "century 1900s panic of 1901 a u s economic recession that started a fight for financial control of the northern pacific\n",
      "railway panic of 1907 a u s economic recession with bank failures 1920s depression of 1920 21 a u s economic recession\n",
      "following the end of xxxunk wall street crash of 1929 and great depression xxxunk the worst depression of modern history\n",
      "1970s 1970s energy crisis opec oil price shock 1973 1979 energy crisis 1979 secondary banking crisis of xxxunk in the uk\n",
      "latin american debt crisis late 1970s early 1980s known as lost decade 1980s early 1980s recession chilean crisis of\n",
      "1982 bank stock crisis israel 1983 japanese asset price bubble xxxunk black monday 1987 1987 us savings and loan crisis\n",
      "failure of 1 xxxunk out of the 3 234 s ls from 1986 to 1995 in the u s 1990s special period in cuba xxxunk early 1990s\n",
      "recession 1991 india economic crisis finnish banking crisis 1990s 1991 1993 swedish banking crisis 1990s 1994 economic\n",
      "crisis in mexico 1997 asian financial crisis 1998 russian financial crisis 1998 99 ecuador financial crisis argentine\n",
      "economic crisis xxxunk xxxunk effect 1999 brazil 21st century 2000s early 2000s recession dot com bubble 2000 2002 us\n",
      "2001 turkish economic crisis 2002 uruguay banking crisis venezuelan general strike of xxxunk 2007 2009 financial crisis\n",
      "late 2000s recession worldwide 2000s energy crisis 2003 2009 oil price bubble subprime mortgage crisis us 2007 2010\n",
      "united states housing bubble and united states housing market correction us 2003 2011 automotive industry crisis of\n",
      "2008–2010 us 2008–2012 icelandic financial crisis 2008–2010 irish banking crisis russian financial crisis of 2008–2009\n",
      "2008 latvian financial crisis venezuelan banking crisis of xxxunk 2008 16 spanish financial crisis 2010s european\n",
      "sovereign debt crisis eu 2009 2019 greek government debt crisis 2009 2019 2010 14 portuguese financial crisis crisis in\n",
      "venezuela 2012 ukrainian crisis 2013 2014 2014 russian financial crisis 2014 2017 brazilian economic crisis 2015 chinese\n",
      "stock market crash turkish currency and debt crisis 2018 see also financial crisis and economic collapse currency crisis\n",
      "hyperinflation and devaluation banking crisis credit xxxunk bank run savings and loan crisis balance of payments crisis\n",
      "depression economics recession stagflation jobless recovery economic bubble stock market bubble and real estate bubble\n",
      "market correction nominal price equilibrium price kondratiev wave business cycle and business cycle models boom and bust\n",
      "fictitious capital intrinsic value speculation crisis theory tendency of the rate of profit to fall reserve army of\n",
      "labour overproduction xxxunk and demand shortfall consolidation business industrial consolidation market concentration\n",
      "capital flight capital strike urban blight deindustrialization wage price spiral list of banking crises references\n",
      "galbraith j k 1990 a short history of financial euphoria new york penguin books isbn 0 xxxunk xxxunk 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=np.random.randint(len(X_arr))\n",
    "\n",
    "print(f'Text #{i} - {dss[\"categories\"][y_arr[i]]}')\n",
    "s = ' '.join(id_to_word[id] for id in X_arr[i])\n",
    "print('\\n'.join(textwrap.wrap(s, width=120, replace_whitespace=False)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_wiki = 'https://drive.google.com/uc?export=download&id=1iteaiSPd1OLJdKZitcv76LzrD_mAC8S8'# 5\n",
    "url_wiki = 'https://drive.google.com/uc?export=download&id=15hhpN2EszdRx7-PN43yGFsoFPuZzwFO1'   # 200\n",
    "url_wiki = 'https://drive.google.com/file/d/1HZxMVthzG-tboMXO0qsPkeH2Cuq1YCbi/view?usp=sharing' # 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-484-9ff0aad810f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_wiki\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Dataset of {len(dss['X'])} texts of {len(set(dss['y']))} classes - vocabulary of {len(dss['word_index'])} words\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'categories'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf20\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf20\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf20\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \"\"\"\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf20\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "f = urllib.request.urlopen(url_wiki)\n",
    "dss = json.load(f)\n",
    "print(f\"Dataset of {len(dss['X'])} texts of {len(set(dss['y']))} classes - vocabulary of {len(dss['word_index'])} words\")\n",
    "print(dss['categories'])\n",
    "      \n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
